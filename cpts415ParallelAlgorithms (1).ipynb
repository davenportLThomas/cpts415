{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPT_S 415 Youtube Dataset Project\n",
    "### Matt Clark, Thomas Davenport, Amir Gholami\n",
    "\n",
    "The purpose of this project was to run multiple implementations of PageRank algorithms on a Dataset of YouTube videos collected in 2007 and compare them with eachother as well as our fond memories of better times. After experimenting with this project, 2007 was truly the Golden Age of the Internet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.files',\n",
       "  'file:///Users/yujingzhu/.ivy2/jars/graphframes_graphframes-0.8.1-spark3.0-s_2.12.jar,file:///Users/yujingzhu/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'PySparkShell'),\n",
       " ('spark.driver.port', '52091'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///Users/yujingzhu/.ivy2/jars/graphframes_graphframes-0.8.1-spark3.0-s_2.12.jar,file:///Users/yujingzhu/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.driver.maxResultSize', '4G'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.submit.pyFiles',\n",
       "  '/Users/yujingzhu/.ivy2/jars/graphframes_graphframes-0.8.1-spark3.0-s_2.12.jar,/Users/yujingzhu/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.jars',\n",
       "  'file:///Users/yujingzhu/.ivy2/jars/graphframes_graphframes-0.8.1-spark3.0-s_2.12.jar,file:///Users/yujingzhu/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.driver.host', 'yujings-air.lan'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.memory', '4G'),\n",
       " ('spark.app.id', 'local-1607897570633'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark import *\n",
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File/Schema Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    }
   ],
   "source": [
    "files = ['0222','0305','0314','0320','0329','0418','0426','0502','0511',\n",
    "        '0301','0306','0315','0322','0403','0420','0428','0505','0513',\n",
    "        '0302','0309','0316','0325','0410','0422','0430','0507','0515',\n",
    "        '0303','0313','0318','0327','0413','0424','0509','0518',\n",
    "        '080327','080329','080331','080402','080514','080623',\n",
    "        '080404','080516','080625','080406','080518','080627',\n",
    "        '080408','080520','080629','080412','080522','080701',\n",
    "        '080414','080524','080703','080416','080526','080705',\n",
    "        '080418','080528','080707','080422','080530','080709',\n",
    "        '080424','080601','080711','080426','080603','080713',\n",
    "        '080428','080605','080715','080430','080609','080717',\n",
    "        '080502','080611','080719','080504','080613','080721',\n",
    "        '080506','080615','080723','080327','080508','080617','080725',\n",
    "        '080329','080510','080619','080727','080331','080512','080621',]   # these are the folder names that contain .txt documents\n",
    "print(len(files))\n",
    "\n",
    "path = 'file:///Users/yujingzhu/Downloads/CPTS415PROJ/'\n",
    "schema = StructType([\n",
    "    StructField(\"videoID\", StringType()),\n",
    "    StructField(\"uploader\", StringType()),\n",
    "    StructField(\"age\", IntegerType()),\n",
    "    StructField(\"cateory\", StringType()),\n",
    "    StructField(\"length\", IntegerType()),\n",
    "    StructField(\"views\", IntegerType()),\n",
    "    StructField(\"rate\", FloatType()),\n",
    "    StructField(\"ratings\", IntegerType()),\n",
    "    StructField(\"comments\", IntegerType()),\n",
    "    StructField(\"rec1\", StringType()),\n",
    "    StructField(\"rec2\", StringType()),\n",
    "    StructField(\"rec3\", StringType()),\n",
    "    StructField(\"rec4\", StringType()),\n",
    "    StructField(\"rec5\", StringType()),\n",
    "    StructField(\"rec6\", StringType()),\n",
    "    StructField(\"rec7\", StringType()),\n",
    "    StructField(\"rec8\", StringType()),\n",
    "    StructField(\"rec9\", StringType()),\n",
    "    StructField(\"rec10\", StringType()),\n",
    "    StructField(\"rec11\", StringType()),\n",
    "    StructField(\"rec12\", StringType()),\n",
    "    StructField(\"rec13\", StringType()),\n",
    "    StructField(\"rec14\", StringType()),\n",
    "    StructField(\"rec15\", StringType()),\n",
    "    StructField(\"rec16\", StringType()),\n",
    "    StructField(\"rec17\", StringType()),\n",
    "    StructField(\"rec18\", StringType()),\n",
    "    StructField(\"rec19\", StringType()),\n",
    "    StructField(\"rec20\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialized Comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|         id|          pagerank|\n",
      "+-----------+------------------+\n",
      "|uBDXh1nHIUg| 277.4531533437353|\n",
      "|qqU_uh4QxRY| 274.8013551210237|\n",
      "|WSXAfmR2LVc|157.25308297188175|\n",
      "|-nidMxvnWko|148.52952769477184|\n",
      "|Hd2vZ6dDN0g|125.11503977089039|\n",
      "|ZJdfjo4HYII|118.33500319834721|\n",
      "|drylZu9_OWU|105.95901984529792|\n",
      "|O40jJXVow2g|100.38387783677888|\n",
      "|bPnARnCDgHE| 93.75760652608402|\n",
      "|z6r8AyWmPuw| 82.40444045260891|\n",
      "|G_MsWsVZChs| 81.64879700331704|\n",
      "|EJZQeelvgo8| 79.43639229240529|\n",
      "|SeVwUrEuxhI| 75.07726908667429|\n",
      "|m3A5FsE9RzI| 69.09430757886147|\n",
      "|0EpuGL3Z56w| 68.28787717755466|\n",
      "|SOAGE2kb10Q|  60.2915284404163|\n",
      "|W41T841OZTg|59.614767495290764|\n",
      "|5ccMkUtjqtM|  59.6058660601915|\n",
      "|xyIGvlHxoEw| 58.99831508818863|\n",
      "|NdQLPQWtBzw| 58.10278695029311|\n",
      "+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------+------------------+\n",
      "|         id|          pagerank|\n",
      "+-----------+------------------+\n",
      "|uBDXh1nHIUg| 277.4531533437353|\n",
      "|qqU_uh4QxRY| 274.8013551210237|\n",
      "|WSXAfmR2LVc|157.25308297188175|\n",
      "|-nidMxvnWko|148.52952769477184|\n",
      "|Hd2vZ6dDN0g|125.11503977089039|\n",
      "|ZJdfjo4HYII|118.33500319834721|\n",
      "|drylZu9_OWU|105.95901984529792|\n",
      "|O40jJXVow2g|100.38387783677888|\n",
      "|bPnARnCDgHE| 93.75760652608402|\n",
      "|z6r8AyWmPuw| 82.40444045260891|\n",
      "|G_MsWsVZChs| 81.64879700331704|\n",
      "|EJZQeelvgo8| 79.43639229240529|\n",
      "|SeVwUrEuxhI| 75.07726908667429|\n",
      "|m3A5FsE9RzI| 69.09430757886147|\n",
      "|0EpuGL3Z56w| 68.28787717755466|\n",
      "|SOAGE2kb10Q|  60.2915284404163|\n",
      "|W41T841OZTg|59.614767495290764|\n",
      "|5ccMkUtjqtM|  59.6058660601915|\n",
      "|xyIGvlHxoEw| 58.99831508818863|\n",
      "|NdQLPQWtBzw| 58.10278695029311|\n",
      "+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------+------------------+\n",
      "|         id|          pagerank|\n",
      "+-----------+------------------+\n",
      "|uBDXh1nHIUg|277.45315334373527|\n",
      "|qqU_uh4QxRY| 274.8013551210236|\n",
      "|WSXAfmR2LVc|157.25308297188172|\n",
      "|-nidMxvnWko|148.52952769477182|\n",
      "|Hd2vZ6dDN0g|125.11503977089036|\n",
      "|ZJdfjo4HYII|118.33500319834718|\n",
      "|drylZu9_OWU| 105.9590198452979|\n",
      "|O40jJXVow2g|100.38387783677886|\n",
      "|bPnARnCDgHE|   93.757606526084|\n",
      "|z6r8AyWmPuw|  82.4044404526089|\n",
      "|G_MsWsVZChs| 81.64879700331703|\n",
      "|EJZQeelvgo8| 79.43639229240527|\n",
      "|SeVwUrEuxhI| 75.07726908667428|\n",
      "|m3A5FsE9RzI| 69.09430757886146|\n",
      "|0EpuGL3Z56w| 68.28787717755465|\n",
      "|SOAGE2kb10Q|60.291528440416286|\n",
      "|W41T841OZTg| 59.61476749529075|\n",
      "|5ccMkUtjqtM| 59.60586606019149|\n",
      "|xyIGvlHxoEw|58.998315088188626|\n",
      "|NdQLPQWtBzw|  58.1027869502931|\n",
      "+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------+------------------+\n",
      "|         id|          pagerank|\n",
      "+-----------+------------------+\n",
      "|uBDXh1nHIUg| 277.4531533437354|\n",
      "|qqU_uh4QxRY| 274.8013551210237|\n",
      "|WSXAfmR2LVc|157.25308297188178|\n",
      "|-nidMxvnWko|148.52952769477187|\n",
      "|Hd2vZ6dDN0g| 125.1150397708904|\n",
      "|ZJdfjo4HYII|118.33500319834722|\n",
      "|drylZu9_OWU|105.95901984529795|\n",
      "|O40jJXVow2g|100.38387783677891|\n",
      "|bPnARnCDgHE| 93.75760652608403|\n",
      "|z6r8AyWmPuw| 82.40444045260892|\n",
      "|G_MsWsVZChs| 81.64879700331706|\n",
      "|EJZQeelvgo8|  79.4363922924053|\n",
      "|SeVwUrEuxhI|  75.0772690866743|\n",
      "|m3A5FsE9RzI| 69.09430757886149|\n",
      "|0EpuGL3Z56w| 68.28787717755466|\n",
      "|SOAGE2kb10Q| 60.29152844041631|\n",
      "|W41T841OZTg| 59.61476749529077|\n",
      "|5ccMkUtjqtM| 59.60586606019151|\n",
      "|xyIGvlHxoEw| 58.99831508818864|\n",
      "|NdQLPQWtBzw| 58.10278695029312|\n",
      "+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------+------------------+\n",
      "|         id|          pagerank|\n",
      "+-----------+------------------+\n",
      "|uBDXh1nHIUg| 277.4531533437353|\n",
      "|qqU_uh4QxRY| 274.8013551210237|\n",
      "|WSXAfmR2LVc|157.25308297188175|\n",
      "|-nidMxvnWko|148.52952769477184|\n",
      "|Hd2vZ6dDN0g|125.11503977089039|\n",
      "|ZJdfjo4HYII|118.33500319834721|\n",
      "|drylZu9_OWU|105.95901984529792|\n",
      "|O40jJXVow2g|100.38387783677888|\n",
      "|bPnARnCDgHE| 93.75760652608402|\n",
      "|z6r8AyWmPuw| 82.40444045260891|\n",
      "|G_MsWsVZChs| 81.64879700331704|\n",
      "|EJZQeelvgo8| 79.43639229240529|\n",
      "|SeVwUrEuxhI| 75.07726908667429|\n",
      "|m3A5FsE9RzI| 69.09430757886147|\n",
      "|0EpuGL3Z56w| 68.28787717755466|\n",
      "|SOAGE2kb10Q|  60.2915284404163|\n",
      "|W41T841OZTg|59.614767495290764|\n",
      "|5ccMkUtjqtM|  59.6058660601915|\n",
      "|xyIGvlHxoEw| 58.99831508818863|\n",
      "|NdQLPQWtBzw| 58.10278695029311|\n",
      "+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------+------------------+\n",
      "|         id|          pagerank|\n",
      "+-----------+------------------+\n",
      "|uBDXh1nHIUg| 277.4531533437353|\n",
      "|qqU_uh4QxRY| 274.8013551210237|\n",
      "|WSXAfmR2LVc|157.25308297188175|\n",
      "|-nidMxvnWko|148.52952769477184|\n",
      "|Hd2vZ6dDN0g|125.11503977089039|\n",
      "|ZJdfjo4HYII|118.33500319834721|\n",
      "|drylZu9_OWU|105.95901984529792|\n",
      "|O40jJXVow2g|100.38387783677888|\n",
      "|bPnARnCDgHE| 93.75760652608402|\n",
      "|z6r8AyWmPuw| 82.40444045260891|\n",
      "|G_MsWsVZChs| 81.64879700331704|\n",
      "|EJZQeelvgo8| 79.43639229240529|\n",
      "|SeVwUrEuxhI| 75.07726908667429|\n",
      "|m3A5FsE9RzI| 69.09430757886147|\n",
      "|0EpuGL3Z56w| 68.28787717755466|\n",
      "|SOAGE2kb10Q|  60.2915284404163|\n",
      "|W41T841OZTg|59.614767495290764|\n",
      "|5ccMkUtjqtM|  59.6058660601915|\n",
      "|xyIGvlHxoEw| 58.99831508818863|\n",
      "|NdQLPQWtBzw| 58.10278695029311|\n",
      "+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------+------------------+\n",
      "|         id|          pagerank|\n",
      "+-----------+------------------+\n",
      "|uBDXh1nHIUg| 277.4531533437353|\n",
      "|qqU_uh4QxRY| 274.8013551210237|\n",
      "|WSXAfmR2LVc|157.25308297188175|\n",
      "|-nidMxvnWko|148.52952769477184|\n",
      "|Hd2vZ6dDN0g|125.11503977089039|\n",
      "|ZJdfjo4HYII|118.33500319834721|\n",
      "|drylZu9_OWU|105.95901984529792|\n",
      "|O40jJXVow2g|100.38387783677888|\n",
      "|bPnARnCDgHE| 93.75760652608402|\n",
      "|z6r8AyWmPuw| 82.40444045260891|\n",
      "|G_MsWsVZChs| 81.64879700331704|\n",
      "|EJZQeelvgo8| 79.43639229240529|\n",
      "|SeVwUrEuxhI| 75.07726908667429|\n",
      "|m3A5FsE9RzI| 69.09430757886147|\n",
      "|0EpuGL3Z56w| 68.28787717755466|\n",
      "|SOAGE2kb10Q|  60.2915284404163|\n",
      "|W41T841OZTg|59.614767495290764|\n",
      "|5ccMkUtjqtM|  59.6058660601915|\n",
      "|xyIGvlHxoEw| 58.99831508818863|\n",
      "|NdQLPQWtBzw| 58.10278695029311|\n",
      "+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------+------------------+\n",
      "|         id|          pagerank|\n",
      "+-----------+------------------+\n",
      "|uBDXh1nHIUg| 277.4531533437353|\n",
      "|qqU_uh4QxRY| 274.8013551210237|\n",
      "|WSXAfmR2LVc|157.25308297188175|\n",
      "|-nidMxvnWko|148.52952769477184|\n",
      "|Hd2vZ6dDN0g|125.11503977089039|\n",
      "|ZJdfjo4HYII|118.33500319834721|\n",
      "|drylZu9_OWU|105.95901984529792|\n",
      "|O40jJXVow2g|100.38387783677888|\n",
      "|bPnARnCDgHE| 93.75760652608402|\n",
      "|z6r8AyWmPuw| 82.40444045260891|\n",
      "|G_MsWsVZChs| 81.64879700331704|\n",
      "|EJZQeelvgo8| 79.43639229240529|\n",
      "|SeVwUrEuxhI| 75.07726908667429|\n",
      "|m3A5FsE9RzI| 69.09430757886147|\n",
      "|0EpuGL3Z56w| 68.28787717755466|\n",
      "|SOAGE2kb10Q|  60.2915284404163|\n",
      "|W41T841OZTg|59.614767495290764|\n",
      "|5ccMkUtjqtM|  59.6058660601915|\n",
      "|xyIGvlHxoEw| 58.99831508818863|\n",
      "|NdQLPQWtBzw| 58.10278695029311|\n",
      "+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "3min 18s ± 19.6 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "comp = path + files[0] +'/4.txt'\n",
    "compdf = spark.read.csv(comp, sep=r'\\t', header = False, schema = schema)\n",
    "\n",
    "noNull = compdf.where(compdf.videoID != 'null') # filtering out videoIDs that are null\n",
    "noNull.createOrReplaceTempView(\"compDF\") # allows to view as SQL table \n",
    "cedges1 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec1 as Reccomends\n",
    "        FROM compDF\n",
    "        \"\"\")\n",
    "cedges2 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec2 as Reccomends\n",
    "        FROM compDF\n",
    "        \"\"\")\n",
    "\n",
    "cedges3 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec3 as Reccomends\n",
    "        FROM compDF\n",
    "        \"\"\")\n",
    "\n",
    "cedges4 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec4 as Reccomends\n",
    "        FROM compDF\n",
    "        \"\"\")\n",
    "\n",
    "cedges5 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec5 as Reccomends\n",
    "        FROM compDF\n",
    "        \"\"\")\n",
    "\n",
    "cedges6 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec6 as Reccomends\n",
    "        FROM compDF\n",
    "        \"\"\")\n",
    "\n",
    "cedges7 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec7 as Reccomends\n",
    "        FROM compDF\n",
    "        \"\"\")\n",
    "\n",
    "cedges8 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec8 as Reccomends\n",
    "        FROM compDF\n",
    "        \"\"\")\n",
    "\n",
    "cedges9 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec9 as Reccomends\n",
    "        FROM compDF\n",
    "        \"\"\")\n",
    "\n",
    "cedges10 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec10 as Reccomends\n",
    "        FROM compDF\n",
    "        \"\"\")\n",
    "\n",
    "cedges11 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec11 as Reccomends\n",
    "        FROM compDF\n",
    "        \"\"\")\n",
    "\n",
    "cedges12 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec12 as Reccomends\n",
    "        FROM compDF\n",
    "        \"\"\")\n",
    "\n",
    "cedges13 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec13 as Reccomends\n",
    "        FROM compDF\n",
    "        \"\"\")\n",
    "\n",
    "cedges14 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec14 as Reccomends\n",
    "        FROM compDF\n",
    "        \"\"\")\n",
    "\n",
    "cedges15 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec15 as Reccomends\n",
    "        FROM compDF\n",
    "        \"\"\")\n",
    "\n",
    "cedges16 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec16 as Reccomends\n",
    "        FROM compDF\n",
    "        \"\"\")\n",
    "\n",
    "cedges17 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec17 as Reccomends\n",
    "        FROM compDF\n",
    "        \"\"\")\n",
    "\n",
    "cedges18 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec18 as Reccomends\n",
    "        FROM compDF\n",
    "        \"\"\")\n",
    "\n",
    "cedges19 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec19 as Reccomends\n",
    "        FROM compDF\n",
    "        \"\"\")\n",
    "\n",
    "cedges20 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec20 as Reccomends\n",
    "        FROM compDF\n",
    "        \"\"\")\n",
    "\n",
    "compedges = cedges1.union(cedges2).union(cedges3).union(cedges4).union(cedges5).union(cedges6).union(cedges7).union(cedges8).union(cedges9).union(cedges10).union(cedges11).union(cedges12).union(cedges13).union(cedges14).union(cedges15).union(cedges16).union(cedges17).union(cedges18).union(cedges19).union(cedges20)\n",
    "compedges.createOrReplaceTempView('compedges')\n",
    "sqlcomp = spark.sql(\n",
    "     \"\"\"\n",
    "       SELECT videoID as src, Reccomends as dst\n",
    "       FROM compedges\n",
    "        \"\"\")\n",
    "\n",
    "cvertices = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT DISTINCT videoID as id\n",
    "    FROM compdf\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "cnoNullEdges = sqlcomp.where(sqlcomp.dst != 'null')\n",
    "\n",
    "c = cvertices\n",
    "d = cnoNullEdges\n",
    "\n",
    "compar= GraphFrame(c, d)\n",
    "\n",
    "comparisonresults = compar.pageRank(resetProbability= 0.15, maxIter = 3)\n",
    "\n",
    "comparisonresults.vertices.createOrReplaceTempView(\"cresults\")\n",
    "test =  spark.sql(\n",
    "                \"\"\"\n",
    "                SELECT *\n",
    "                FROM cresults\n",
    "                ORDER BY pagerank desc\n",
    "                \"\"\"\n",
    ")\n",
    "\n",
    "test.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in and Joining 2007 Data\n",
    "       We also removed any possible null videoID tuples to reduce size of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "+-----------+----------------+----+----------------+------+------+----+-------+--------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "|    videoID|        uploader| age|         cateory|length| views|rate|ratings|comments|       rec1|       rec2|       rec3|       rec4|       rec5|       rec6|       rec7|       rec8|       rec9|      rec10|      rec11|      rec12|      rec13|      rec14|      rec15|      rec16|      rec17|      rec18|      rec19|      rec20|\n",
      "+-----------+----------------+----+----------------+------+------+----+-------+--------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "|1xbSFrHzFQ0|        Ireton06| 482|Film & Animation|   245|  9780| 4.0|      9|       6|7vNsnB_qFGA|ndnlUZxB1pg|y8AkKnLMELo|aLAIaFdDHUQ|tY7XxpwDd_8|Yl_A7r1D_OE|YBSoLnN9AiM|wKecR3arrnQ|MG6NYhMRCgs|VvnSYenJRVQ|krtueF7CnCs|hZ0-sKxr_8Y|bI53cIMWjLQ|jC0Kdp8AQK8|x0fN3DYfIF0|TGfmE6NlIe0|BaBLJv3jQMI|OC8jUahyJbM|NwqBo9J2Ya0|2FD6kR5AqG4|\n",
      "|4VP4qSjDNQs|    themaster095| 718|Film & Animation|   528|    17| 0.0|      0|       0|FJXOBibQLpc|EnPxBagNn1k|pWFpu0pS5w8|q6w5hPbzMvc|HEMv0JGT_w0|pn9KCxQpxIA|hGUFra04-Go|PPc4Ej4TS18|9Ph_0sXd-pQ|yM_hbsGjb5s|BPgxvlM6j6I|32pbWDmM_gY|c5c6EDSnXOE|7_dL8ryvixE|sdkpVwEMrz0|l4GD8Ea-zl0|qyTbVmt-eFY|Gs-FmY9JI9o|2uANjyuOv-I|Pic-eGLctY4|\n",
      "|RJgGeYiJrj0|   SpecialtyActs| 526|   Entertainment|    82|  1156|4.33|      9|       2|G2dE3AEGFhY|3TdnDKdg5TA|dFhKfEOQm0E|XYaCuq_Y1y0|Z3E96UmK8As|jO2mT9JAuH8|UDAWdEUoaII|EW7mt5u8iWQ|wyRyrqNbW1U|lLwNHprgHv8|VqI0w0VStJ0|vZx9UVxg7r8|QjDhjXG91LY|XmHNqe23P0k|L67SFTqBeXo|p07zGg1sQkw|lon4jR8YSv0|hvvmopDfWq4|bh0UDi8t4Fo|FsxZbaxut2w|\n",
      "|MqySp7Nq5j0|wumingfoundation| 614|   Entertainment|   688| 93611|4.28|     64|      27|7aLwNXyn-Oo|lE_IxGjmtdM|SFdlbq8RZMo|PLi81AiAOUI|1VkVRlUn5s4|27On1afCOnM|HkwIFuNjFXo|loeIV4RLw9M|Vatw8Ru5aUY|UOqkdnF-9-s|mWnENjYuYRQ|9-qA_OeIUH4|kifKMiKpDRA|7kJXoFh8ccM|YIJjS2sCDpA|EPX0eHNeBLc|CkAmZK9UzWg|oAgTgCK0ETU|9g7-UD_mpOs|aAl1uQ9JqU8|\n",
      "|MC--VwYTHAM|     sk8ter14475| 427|Film & Animation|   248| 51820|4.39|    109|      72|AFUynmUi-ok|DcLD7OoEzkI|sEII_trLOIE|GrusMyXfgiY|OtFpiXkX_mo|zpStscOTU9s|22Zs65Pvzto|X8SDpNM4Yko|fkkDESIjyhY|n8bBFdlFCUc|ZDHSbrPZH4A|t2lEk4PQIs4|vtgPAQTJLQs|l9FA3KK3VEY|WcGzpWzLDcU|i0kouCaxBSU|VPrcuNvtNUk|nYprMNxcX4w|9W2dzhNyglo|FV6nJxg7mM0|\n",
      "|eVdiIbWT60M|          m4df0x|null|            UNA |   171| 84754|4.29|     84|      43|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|\n",
      "|Da80HD18tp0|       myemma100| 468|Film & Animation|   224|163183|4.39|    187|      70|MqySp7Nq5j0|v2uEfmWO6z8|MC--VwYTHAM|4SSZBAPleVc|eVdiIbWT60M|tdRBH7VBrSY|LvIDRoO8KnM|1ZU_ytaZTxg|xKvzxLeY0iQ|gompU_uhYq0|mhfp6Z8z1cI|CiPPxBxPBOw|MeFi3SDi_n8|oV0cDx_gS_A|YUhlHMb1qKU|_Wud5vSIQ0I|j1oGIffyVVk|PLkQHjgD3rg|Eja6kvGN5v0|X7woJN8KNqM|\n",
      "|mhfp6Z8z1cI|    puntadeleste| 566|Film & Animation|   512|  6970|4.87|     23|      13|HeIApq5MeTw|9CdsANAXwBM|j_WMrTkM1hs|cxRudB_XXPc|MqySp7Nq5j0|MC--VwYTHAM|4SSZBAPleVc|eVdiIbWT60M|Da80HD18tp0|tdRBH7VBrSY|LvIDRoO8KnM|xKvzxLeY0iQ|1ZU_ytaZTxg|gompU_uhYq0|CiPPxBxPBOw|oV0cDx_gS_A|YUhlHMb1qKU|MeFi3SDi_n8|YRi20cWMYOM|PLkQHjgD3rg|\n",
      "|tdRBH7VBrSY|     disneykid23| 438|   Entertainment|   253| 35785|4.65|    138|      51|zvd_BUWDnSA|zWPHVgM-H8I|Y1E8kCVadAw|OhDuVwe6Rto|N_HsOsigEjo|DAR4ZNWBYf0|yNOwc6UjYwU|Lgxzcld-LLM|bTgZREEHiDo|MNKrPTZRq48|cUDyYo8gN3c|1QwD5CMF1IQ|bXno7Bb4-Fk|X5mx1YjIWI0|nzd09OUSBl8|gapch5NTTkQ|OcBnOkx8wd8|zPKKh2nYqSA|jsKHMrPR5z8|83yEoL6qf2w|\n",
      "|xKvzxLeY0iQ|       AnimeAnge| 516|   Entertainment|   474| 12724|4.69|     55|      18|ojg0bqCuHiw|xBb3gArg90g|giTgzsFjm3Q|BuwzqTut3nY|5o00znI21oI|1W3XK7ujVxU|x7iUl_bPL4M|XCzqRA33tXc|OchvLYpTf38|JMOoJwTfr6c|4ickYn3BXUc|RIGmBPhg3c8|RyuTVGFSUOc|9Sg_ENsXYP0|5q_I2GQlLTw|uPggxeEuRBU|Yan4eAx-W-Q|fPgWPpQELi4|8C81Zwsyk_o|nJcRpHQDQC0|\n",
      "|_I2EZYCdUXI|        magogami| 334|          Comedy|   144| 53371| 4.8|    327|      95|VpAFZrXHmus|H1GH92mv1cg|fpzBXcINJIM|83mcABtS3eo|34Io83aeMq4|4fjQ3gd5Kn8|3cL1uLcaG3w|w3-WzqfbVHU|EwIbz9SEzRI|4l5etjhNyUI|rRvKODfqOLs|FYRH2jp2wzs|ALJVhTOiL-E|7putw2A4FiI|KgOhVM7GN5o|N0iK7WFqGWk|aTUsz_9B9RY|-1JYvjetABs|7P1l0FoX6e0|qnNbleh6s10|\n",
      "|gompU_uhYq0|     unclejimboy| 491|Film & Animation|   211| 37341|4.55|     22|      21|FTQlGJF6TRQ|LiVa2vsYghE|t7GJUB7XWtQ|4op9TYRP1yI|-pERlfYNT_Q|A7vRhyrtNkY|N75xU3RiSPo|wfJhuh8_-GA|r5adby3uKO4|v6fRoaUpN7U|E-fK6bXEfZY|jAXc8d1NmLE|W2LHkU2dW8M|BPCFHKdxDjQ|75dhTlZcW9Y|0Qcr3m7SaNI|VwZ44CE4jJY|SnyDZ0uGPsQ|xIEaZDJqc5w|BeYPqR9aw00|\n",
      "|CiPPxBxPBOw|        jra81984| 691| News & Politics|   124|220883|3.36|    279|    1089|O0xoEtCHgA8|ns22hXjb1Zc|S4Z3hjPuzJ0|uDojz-rb8Ec|zaeaEZ1ZceI|7CGeiFTdits|uMG-9ubqDtk|eulT4m7WX3k|yuDT7-5RwAE|STptWddu4oA|FvkcNnCbMWA|Vsj99WaAr_Q|I3sE58IDKXI|qVUjsgUDnAk|-NvAS6XDWO4|p0ifdL7aegg|59VWIAzXsxE|MliIh16yDd4|zYS5VFvSMws|VeXbId2P6Yc|\n",
      "|YRi20cWMYOM|   incubusfanclm| 452|           Music|   148|269290|4.88|    854|     248|8FsGqhbBb8w|G-e06GNDBsY|Tdc1KVjKWng|KrhWjOSINFM|iZY2HlmKPy8|LTvu7Qs50w8|tIVvG_xz2sw|9R9oCMRT-uk|cQ-l0VnQMEg|5XallJsDgj4|qi8ymxhyXok|QjzQx3tkE3o|sl7BQnDg7tE|8JSRS6Syo9E|NYEwcjXzLwQ|Fd-U2EXTvhs|TINPobJVFEc|CM370UtN-cs|cac5buQfibU|KdSeSYamkgY|\n",
      "|v2uEfmWO6z8|          mimzi8| 601|Film & Animation|   361| 25341|4.63|     78|      44|AItaLnpbIAw|0X_of0woMBU|yJ0s-kwG_Ro|qThc1spQuiY|WCEH9RZZKdU|8ZgFpRZeCds|PQfyKAo0-ls|5peppnUHf9I|Dx3ri8EVEvE|ofmQOjZYLDI|Ndtp2wP31K4|HEo6U-jkD4I|3DG1XyqgX3w|b7M2mrnSZpM|5gJ21l8_4II|MqySp7Nq5j0|MC--VwYTHAM|4SSZBAPleVc|eVdiIbWT60M|Da80HD18tp0|\n",
      "|2t2Fe_ixWpI|        arorange| 277|  People & Blogs|   229|118356|4.39|    124|      55|MqySp7Nq5j0|v2uEfmWO6z8|MC--VwYTHAM|4SSZBAPleVc|eVdiIbWT60M|Da80HD18tp0|tdRBH7VBrSY|LvIDRoO8KnM|1ZU_ytaZTxg|xKvzxLeY0iQ|gompU_uhYq0|mhfp6Z8z1cI|CiPPxBxPBOw|MeFi3SDi_n8|oV0cDx_gS_A|YUhlHMb1qKU|j1oGIffyVVk|PLkQHjgD3rg|Eja6kvGN5v0|X7woJN8KNqM|\n",
      "|bBml9opQxnc|    OnRideVideos| 520| Travel & Places|   456| 32690|4.71|    114|      82|IGCHodIZrd4|ZO2M6Eueks0|Fr2WGH_zEjg|csIy1_V37YM|md6rjuJ00mE|9ZGYPvlcX-E|eVwrfC1o3zA|CpmUCcOhP3Y|sNCz66RfNik|3l8bgbGbtYU|WxtzxebKSX4|Oxr7Ivr92_w|GWGdZzof3yw|_sF5U8CUJZU|KscJb3YSuwA|OZ_wBnsJ5KA|uUcUE1NvWmg|OiHvBql4w3s|SkW11ZdOb0w|eVC_hW28dng|\n",
      "|1ZU_ytaZTxg|  sparklestar502| 423|           Music|   207|126044|4.76|    551|     147|Y1ZRelAjGKA|kY1tylDMbxM|aac2R-99r9s|Hzs_Fu1O7jo|hffyCy6CjDg|Xy4bneK_GjA|V5x-4CissvU|DzrKA-h76X0|l41EH-A4Mes|WkslaYosKEw|LF9kjt5DFXs|kZwXp_FyIZw|O1pMCWtrvMQ|00-jcFBUlK4|WU7M6cfBwdk|UUJGkXHR6oQ|LWdAuSvW9aM|0f4Pddef9PQ|o07siZPGlXg|nNhcTCNo7c8|\n",
      "|NVRkDihnHB0|       BMcPhurry| 658|           Music|   165|   313| 0.0|      0|       0|XeRBsvGOfJA|OtlsTaOYpW8|BXwWKzUbX1A|f4xYLTgnVLY|uSgJPrr9EKQ|sPYphhaMnmI|SQEGZ1WIyoo|Guu4HU7hin4|HlFC_a-yhUo|Eq-hrO3nJT0|_avMIT7aouA|qcw0-Ym3IBA|QTaaBQ-oEtg|Y6kDAW3aIkg|VdrHvuHYWzc|YMwf9DT_oL8|Lg1MJbNiW7c|y0f0L1GrI2g|AffXeOC4em4|wHZ03-ebDd0|\n",
      "|wL1-yb-vb1o|          twinxy| 428|   Entertainment|   246|  6469|4.27|     41|      37|KRoFp1xpbWQ|qcw0-Ym3IBA|aGS5U37SafQ|RgA_z3DXfFI|mkTWQrkEdTs|DzeJJ30DHtg|Qg50zQfnJdE|yCpjkD6vz-g|NqvtV3ILIWY|-KHggKpoG3g|QjhEnilCIWM|U-quY1hbbmw|zU-4as8z5xg|TC064TClS80|k7-howVC5Uk|rcXc2A9BmSg|hRDZ2wP6XNI|uyHxkcsTnF0|Ew0bMPYoPHo|MQdC_wjSZpM|\n",
      "+-----------+----------------+----+----------------+------+------+----+-------+--------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "############### THIS SHOULD BE A LOOP HOWEVER I CANNOT FIGURE OUT HOW TO UTILIZE A LIST OF RDDS\n",
    "j = 0\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df1 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df2 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df3 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df4 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df5 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df6 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df7 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df8 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df9 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df10 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df11 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df12 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df13 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df14 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df15 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df16 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df17 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df18 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df19 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df20 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df21 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df22 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df23 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df24 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df25 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df26 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df27 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df28 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df29 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df30 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df31 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df32 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df33 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df34 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    " \n",
    "loopath = path + str(files[j]) +'/'\n",
    "df35 = spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j+=1\n",
    "     \n",
    "print('START')\n",
    "\n",
    "finalDF = df1.union(df2).union(df3).union(df4).union(df5).union(df6).union(df7).union(df9).union(df8).union(df10).union(df11).union(df12).union(df13).union(df14).union(df15).union(df16).union(df17).union(df18).union(df19).union(df20).union(df21).union(df22).union(df23).union(df24).union(df25).union(df26).union(df27).union(df28).union(df29).union(df30).union(df31).union(df32).union(df33).union(df34).union(df35)\n",
    "# union all of the df RDDs that were read\n",
    "finalDF.show() # show the final DF\n",
    "noNullDF = finalDF.where(finalDF.videoID != 'null') # filtering out videoIDs that are null\n",
    "noNullDF.createOrReplaceTempView(\"finalDF\") # allows to view as SQL table \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating Edges from DataFrame \n",
    "    PageRank requires a directed graph, this step transforms all recommendation links into a directed edge table, src->dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges1 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec1 as Reccomends\n",
    "        FROM finalDF\n",
    "        \"\"\")\n",
    "edges2 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec2 as Reccomends\n",
    "        FROM finalDF\n",
    "        \"\"\")\n",
    "\n",
    "edges3 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec3 as Reccomends\n",
    "        FROM finalDF\n",
    "        \"\"\")\n",
    "\n",
    "edges4 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec4 as Reccomends\n",
    "        FROM finalDF\n",
    "        \"\"\")\n",
    "\n",
    "edges5 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec5 as Reccomends\n",
    "        FROM finalDF\n",
    "        \"\"\")\n",
    "\n",
    "edges6 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec6 as Reccomends\n",
    "        FROM finalDF\n",
    "        \"\"\")\n",
    "\n",
    "edges7 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec7 as Reccomends\n",
    "        FROM finalDF\n",
    "        \"\"\")\n",
    "\n",
    "edges8 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec8 as Reccomends\n",
    "        FROM finalDF\n",
    "        \"\"\")\n",
    "\n",
    "edges9 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec9 as Reccomends\n",
    "        FROM finalDF\n",
    "        \"\"\")\n",
    "\n",
    "edges10 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec10 as Reccomends\n",
    "        FROM finalDF\n",
    "        \"\"\")\n",
    "\n",
    "edges11 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec11 as Reccomends\n",
    "        FROM finalDF\n",
    "        \"\"\")\n",
    "\n",
    "edges12 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec12 as Reccomends\n",
    "        FROM finalDF\n",
    "        \"\"\")\n",
    "\n",
    "edges13 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec13 as Reccomends\n",
    "        FROM finalDF\n",
    "        \"\"\")\n",
    "\n",
    "edges14 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec14 as Reccomends\n",
    "        FROM finalDF\n",
    "        \"\"\")\n",
    "\n",
    "edges15 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec15 as Reccomends\n",
    "        FROM finalDF\n",
    "        \"\"\")\n",
    "\n",
    "edges16 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec16 as Reccomends\n",
    "        FROM finalDF\n",
    "        \"\"\")\n",
    "\n",
    "edges17 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec17 as Reccomends\n",
    "        FROM finalDF\n",
    "        \"\"\")\n",
    "\n",
    "edges18 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec18 as Reccomends\n",
    "        FROM finalDF\n",
    "        \"\"\")\n",
    "\n",
    "edges19 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec19 as Reccomends\n",
    "        FROM finalDF\n",
    "        \"\"\")\n",
    "\n",
    "edges20 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec20 as Reccomends\n",
    "        FROM finalDF\n",
    "        \"\"\")\n",
    "\n",
    "finaledges = edges1.union(edges2).union(edges3).union(edges4).union(edges5).union(edges6).union(edges7).union(edges8).union(edges9).union(edges10).union(edges11).union(edges12).union(edges13).union(edges14).union(edges15).union(edges16).union(edges17).union(edges18).union(edges19).union(edges20)\n",
    "finaledges.createOrReplaceTempView('finaledges')\n",
    "sqlFinalEdges = spark.sql(\n",
    "     \"\"\"\n",
    "       SELECT videoID as src, Reccomends as dst\n",
    "       FROM finaledges\n",
    "        \"\"\")\n",
    "#sqlFinalEdges.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting distinct VideoIDs \n",
    "    Our data was collected almost daily, so there were some videos who had their data collected multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT DISTINCT videoID as id\n",
    "    FROM finalDF\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing all null values in our Edge Table\n",
    "    Videos can recommend 0-20 videos on YouTube, we removed any null values to decrease sparcity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "noNullEdges = sqlFinalEdges.where(sqlFinalEdges.dst != 'null')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We used GraphFrames PageRank functionality For our Parallel Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import GraphFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = vertices\n",
    "e = noNullEdges\n",
    "#e.cache()\n",
    "#v.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = GraphFrame(v, e)\n",
    "results = g.pageRank(resetProbability= 0.15, maxIter = 3)\n",
    "#results.vertices.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|         id|          pagerank|\n",
      "+-----------+------------------+\n",
      "|H1BYrSk46vg|169.21998815123456|\n",
      "|n6x7jyFAf7E|166.55954282915377|\n",
      "|nypYVhnGbcQ|165.11300878534232|\n",
      "|22uYDgZXoJQ|159.08543680676843|\n",
      "|9nbeqmXEbj8| 155.9309649524114|\n",
      "|JrUupfFzBrA|155.84055734355525|\n",
      "|qcw0-Ym3IBA|155.40648959962954|\n",
      "|CsE1oZX3Eco| 151.6517640032689|\n",
      "|y2jUOABWK1A| 145.1153206847998|\n",
      "|13n8OpQS9Dg|144.32517179075552|\n",
      "|oeGjJ-joZdI|142.79058337594088|\n",
      "|JbQr_7VNIeA| 136.9054140263238|\n",
      "|R7RWQSpArm8|136.74600110052222|\n",
      "|udr9sLkoZ0s| 135.5397200555263|\n",
      "|QLrEcMq_nPQ| 135.3746348347861|\n",
      "|UjYvKPh8geM|135.13248746634085|\n",
      "|dv8eQXeishc|134.17753442913138|\n",
      "|8AInR7EZ-zo|128.56132524038395|\n",
      "|5FQN_MYRm4U|127.82728547505947|\n",
      "|c6VAs8uZjlU|127.39860998828337|\n",
      "+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.vertices.createOrReplaceTempView(\"results\")\n",
    "test =  spark.sql(\n",
    "                \"\"\"\n",
    "                SELECT *\n",
    "                FROM results\n",
    "                ORDER BY pagerank desc\n",
    "                \"\"\"\n",
    ")\n",
    "\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphFrames also has Pregel Model Functionality\n",
    "    Credit: GraphFrames.github.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3182185\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce, col, lit, sum, when\n",
    "from graphframes.lib import Pregel\n",
    "v = vertices\n",
    "e = noNullEdges\n",
    "e.cache()\n",
    "numVertices = v.count()\n",
    "print(numVertices)\n",
    "l = GraphFrame(v, e).outDegrees\n",
    "l.cache()\n",
    "graph = GraphFrame(l, e)\n",
    "alpha = 0.15\n",
    "ranks = graph.pregel \\\n",
    "        .setMaxIter(1) \\\n",
    "        .withVertexColumn(\"rank\", lit(1.0/numVertices), \\\n",
    "                            coalesce(Pregel.msg(), lit(0.0)) * lit(1.0-alpha) + lit(alpha / numVertices)) \\\n",
    "        .sendMsgToDst(Pregel.src(\"rank\") / Pregel.src(\"outDegree\")) \\\n",
    "        .aggMsgs(sum(Pregel.msg())) \\\n",
    "        .run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------------------+\n",
      "|         id|outDegree|                rank|\n",
      "+-----------+---------+--------------------+\n",
      "|22uYDgZXoJQ|       20|3.820185943088657E-5|\n",
      "|dMH0bHeiRNg|       40| 3.51604572342785E-5|\n",
      "|H1BYrSk46vg|       20|3.390903343858859E-5|\n",
      "|pv5zWaTEVkI|       40|3.329189344690189...|\n",
      "|qcw0-Ym3IBA|       40|3.315816168818532E-5|\n",
      "|n6x7jyFAf7E|       40|3.246776192733975E-5|\n",
      "|nypYVhnGbcQ|       20|3.241673543736694E-5|\n",
      "|dv8eQXeishc|       20|3.215098003153404...|\n",
      "|9nbeqmXEbj8|       20|3.188640143881359...|\n",
      "|CsE1oZX3Eco|       40|3.130772169942119...|\n",
      "|vr3x_RRJdd4|       40|3.037148956486398...|\n",
      "|JrUupfFzBrA|       20|3.036570721206525...|\n",
      "|13n8OpQS9Dg|       20|2.983311537205251...|\n",
      "|udr9sLkoZ0s|       40|2.981646718279201...|\n",
      "|y2jUOABWK1A|       20|2.965170513262858E-5|\n",
      "|5FQN_MYRm4U|       20|2.903632293676921E-5|\n",
      "|cQ25-glGRzI|       20|2.891407724128564...|\n",
      "|1dmVU08zVpA|       40|2.821521020700072...|\n",
      "|oeGjJ-joZdI|       20|2.773573772046938...|\n",
      "|QLrEcMq_nPQ|       20|2.757030527562546...|\n",
      "+-----------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ranks.show()\n",
    "ranks.createOrReplaceTempView(\"Pregle\")\n",
    "ay = spark.sql (\n",
    "        \"\"\"\n",
    "        SELECT *\n",
    "        FROM Pregle\n",
    "        ORDER BY rank DESC\n",
    "        \"\"\")\n",
    "ay.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.clearCache()\n",
    "spark.catalog.clearCache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2008 Data\n",
    "    After exploring 2007 data, something wasn't right, something was missing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "j =35\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df36= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df37= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df38= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df39= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df40= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df41= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df42= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df43= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df44= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df45= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df46= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df47= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df48= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df49= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df50= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df51= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df52= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df53= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df54= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df55= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df56= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df57= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df58= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df59= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df60= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df61= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df62= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df63= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df64= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df65= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df66= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df67= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df68= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df69= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df70= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df71= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df72= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df73= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df74= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df75= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df76= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df77= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df78= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df79= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df80= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df81= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df82= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df83= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df84= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df85= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df86= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df87= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df88= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df89= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df90= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df91= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df92= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "loopath = path + str(files[j]) +'/'\n",
    "df93= spark.read.csv(loopath, sep=r'\\t', header=False, schema = schema)\n",
    "j +=1\n",
    "\n",
    "df2008 = df36.union(df37).union(df38).union(df39).union(df40).union(df41).union(df42).union(df43).union(df44).union(df45).union(df46).union(df47).union(df48).union(df49).union(df50).union(df51).union(df52).union(df53).union(df54).union(df55).union(df56).union(df57).union(df58).union(df59).union(df60).union(df61).union(df62).union(df63).union(df64).union(df65).union(df66).union(df67).union(df68).union(df69).union(df70).union(df71).union(df72).union(df73).union(df74).union(df75).union(df76).union(df77).union(df78).union(df79).union(df80).union(df81).union(df82).union(df83).union(df84).union(df85).union(df86).union(df87).union(df88).union(df89).union(df90).union(df91).union(df92).union(df93)\n",
    "noNullDF2008 = df2008.where(df2008.videoID != 'null') # filtering out videoIDs that are null\n",
    "noNullDF2008.createOrReplaceTempView(\"DF2008\") # allows to view as SQL table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = spark.sql(\n",
    "        \"\"\"\n",
    "        SELECT * \n",
    "        FROM DF2008\n",
    "       \n",
    "        \"\"\")\n",
    "subset.createOrReplaceTempView(\"finalDF2008\")\n",
    "\n",
    "\n",
    "e1 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec1 as Reccomends\n",
    "        FROM finalDF2008\n",
    "        \"\"\")\n",
    "e2 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec2 as Reccomends\n",
    "        FROM finalDF2008\n",
    "        \"\"\")\n",
    "\n",
    "e3 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec3 as Reccomends\n",
    "        FROM finalDF2008\n",
    "        \"\"\")\n",
    "\n",
    "e4 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec4 as Reccomends\n",
    "        FROM finalDF2008\n",
    "        \"\"\")\n",
    "\n",
    "e5 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec5 as Reccomends\n",
    "        FROM finalDF2008\n",
    "        \"\"\")\n",
    "\n",
    "e6 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec6 as Reccomends\n",
    "        FROM finalDF2008\n",
    "        \"\"\")\n",
    "\n",
    "e7 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec7 as Reccomends\n",
    "        FROM finalDF2008\n",
    "        \"\"\")\n",
    "\n",
    "e8 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec8 as Reccomends\n",
    "        FROM finalDF2008\n",
    "        \"\"\")\n",
    "\n",
    "e9 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec9 as Reccomends\n",
    "        FROM finalDF2008\n",
    "        \"\"\")\n",
    "\n",
    "e10 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec10 as Reccomends\n",
    "        FROM finalDF2008\n",
    "        \"\"\")\n",
    "\n",
    "e11 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec11 as Reccomends\n",
    "        FROM finalDF2008\n",
    "        \"\"\")\n",
    "\n",
    "e12 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec12 as Reccomends\n",
    "        FROM finalDF2008\n",
    "        \"\"\")\n",
    "\n",
    "e13 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec13 as Reccomends\n",
    "        FROM finalDF2008\n",
    "        \"\"\")\n",
    "\n",
    "e14 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec14 as Reccomends\n",
    "        FROM finalDF2008\n",
    "        \"\"\")\n",
    "\n",
    "e15 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec15 as Reccomends\n",
    "        FROM finalDF2008\n",
    "        \"\"\")\n",
    "\n",
    "e16 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec16 as Reccomends\n",
    "        FROM finalDF2008\n",
    "        \"\"\")\n",
    "\n",
    "e17 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec17 as Reccomends\n",
    "        FROM finalDF2008\n",
    "        \"\"\")\n",
    "\n",
    "e18 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec18 as Reccomends\n",
    "        FROM finalDF2008\n",
    "        \"\"\")\n",
    "\n",
    "e19 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec19 as Reccomends\n",
    "        FROM finalDF2008\n",
    "        \"\"\")\n",
    "\n",
    "e20 = spark.sql(\n",
    "      \"\"\"\n",
    "        SELECT videoID, rec20 as Reccomends\n",
    "        FROM finalDF2008\n",
    "        \"\"\")\n",
    "\n",
    "finaledges2008 = e1.union(e2).union(e3).union(e4).union(e5).union(e6).union(e7).union(e8).union(e9).union(e10).union(e11).union(e12).union(e13).union(e14).union(e15).union(e16).union(e17).union(e18).union(e19).union(e20)\n",
    "finaledges2008.createOrReplaceTempView('finaledges2008')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices2008 = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT DISTINCT videoID as id\n",
    "    FROM finalDF2008\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "sqlFinalEdges2008 = spark.sql(\n",
    "     \"\"\"\n",
    "       SELECT videoID as src, Reccomends as dst\n",
    "       FROM finaledges2008\n",
    "        \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "noNullEdges2008 = sqlFinalEdges2008.where(sqlFinalEdges2008.dst != 'null')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import GraphFrame\n",
    "v2008 = vertices2008\n",
    "e2008 = noNullEdges2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|         id|          pagerank|\n",
      "+-----------+------------------+\n",
      "|-8E02ubcogo|0.8947947237957806|\n",
      "|-9oQzeXGqjU|0.5984020785572196|\n",
      "|36-VneN8Z_E|1.2815266495954107|\n",
      "|7GyKt4RwXQg|0.5684394776588397|\n",
      "|D5_UDa28a4c|1.0123518183246412|\n",
      "|DMBcyt_CRM4|1.2479774681872893|\n",
      "|EGCVRdZ5j78|1.3335862102018008|\n",
      "|FXmEav8fc50|1.0138733380533649|\n",
      "|GHkYUEdqw7U|0.5338633459960733|\n",
      "|HfunkZMKMIo|0.8563770801079119|\n",
      "|I2MsDogV4g4| 2.093710844614686|\n",
      "|JjF8zJncvDo| 1.268926199736484|\n",
      "|KHe1zJHtZw0|1.6059305022987649|\n",
      "|KMU4qk9apGY|0.8542313176432667|\n",
      "|Nfg8Ogu1rFo|1.1022372142969192|\n",
      "|P0MvUZYE1Vg| 1.061553657133882|\n",
      "|T10ZP4CnSBY|0.8354648035682074|\n",
      "|VHDQ2EKmc3g|0.5142488042052193|\n",
      "|c43eXbefnzs|0.5958069913984343|\n",
      "|e-fEFtxe_LI| 1.672271789790411|\n",
      "+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#e2008.cache()\n",
    "#v2008.cache()\n",
    "g2008 = GraphFrame(v2008, e2008)\n",
    "results2008 = g2008.pageRank(resetProbability= 0.15, maxIter = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results2008' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5c4bfd26e748>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults2008\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"results2008\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m results2008ord = spark.sql(\n\u001b[1;32m      3\u001b[0m                 \"\"\"\n\u001b[1;32m      4\u001b[0m                 \u001b[0mSELECT\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mFROM\u001b[0m \u001b[0mresults2008\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results2008' is not defined"
     ]
    }
   ],
   "source": [
    "results2008.vertices.createOrReplaceTempView(\"results2008\")\n",
    "results2008ord = spark.sql(\n",
    "                \"\"\"\n",
    "                SELECT * \n",
    "                FROM results2008\n",
    "                ORDER BY pagerank DESC\n",
    "                \"\"\"\n",
    ")\n",
    "results2008ord.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce, col, lit, sum, when\n",
    "from graphframes.lib import Pregel\n",
    "numVertices = v2008.count()\n",
    "l2008 = GraphFrame(v2008, e2008).outDegrees\n",
    "l2008.cache()\n",
    "graph2008 = GraphFrame(l2008, e2008)\n",
    "alpha = 0.15\n",
    "ranks = graph2008.pregel \\\n",
    "        .setMaxIter(1) \\\n",
    "        .withVertexColumn(\"rank\", lit(1.0/numVertices), \\\n",
    "                            coalesce(Pregel.msg(), lit(0.0)) * lit(1.0-alpha) + lit(alpha / numVertices)) \\\n",
    "        .sendMsgToDst(Pregel.src(\"rank\") / Pregel.src(\"outDegree\")) \\\n",
    "        .aggMsgs(sum(Pregel.msg())) \\\n",
    "        .run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------------------+\n",
      "|         id|outDegree|                rank|\n",
      "+-----------+---------+--------------------+\n",
      "|KMP8OSWGcss|       40|3.517188781869857E-5|\n",
      "|lkRUawKehLA|       40|3.138772710813611E-5|\n",
      "|0rgQho8licc|       28|3.123484744085196E-5|\n",
      "|Ba3N_awXcow|       40|2.816921636817448...|\n",
      "|Ejb8QOyjz-E|       40|2.012231119830615...|\n",
      "|TBrBkS5DA4Y|       40|1.941755865978858...|\n",
      "|v3ARyAb_1Bs|       40|1.863084103317924...|\n",
      "|sF84pIhP5UM|       40|1.831774231911334...|\n",
      "|eBGIQ7ZuuiU|       28|1.802960418214307...|\n",
      "|JR43BD6K7jg|       40|1.783752905594586E-5|\n",
      "|xsRWpK4pf90|       40|1.782953602152783E-5|\n",
      "|UheCchftswc|       40|1.667861459698304E-5|\n",
      "|ktUSIJEiOug|       40| 1.56987923379947E-5|\n",
      "|IG6wgKojMMs|       28|1.494027066055493...|\n",
      "|89oS4SN4mNg|       38|1.475335523363048...|\n",
      "|JznPMfYVWyE|       20|1.461610458653507...|\n",
      "|XEQXIDbz6Oo|       40|1.458416732620963E-5|\n",
      "|8JUvbJekM88|       40|1.439438546676238...|\n",
      "|_wihKvfbFew|       20|1.420786084604458...|\n",
      "|pwGLNbiw1gk|       20|1.409996327466927...|\n",
      "+-----------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ranks.createOrReplaceTempView(\"Pregle2008\")\n",
    "pregel2008 = spark.sql (\n",
    "        \"\"\"\n",
    "        SELECT *\n",
    "        FROM Pregle2008\n",
    "        ORDER BY rank DESC\n",
    "        \"\"\")\n",
    "pregel2008.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our predictions of what videos would have the highest PageRank scores were completely wrong we decided to query for them in each of our tables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chocolate Rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|         id|          pagerank|\n",
      "+-----------+------------------+\n",
      "|EwTZ2xpQwpA|0.9025025920602984|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chocrain = spark.sql (\n",
    "    \"\"\" \n",
    "    SELECT *\n",
    "    FROM results\n",
    "    WHERE id = 'EwTZ2xpQwpA'\n",
    "    \"\"\")\n",
    "\n",
    "chocrain.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------------------+\n",
      "|         id|outDegree|                rank|\n",
      "+-----------+---------+--------------------+\n",
      "|EwTZ2xpQwpA|       20|3.543162952499619E-7|\n",
      "+-----------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chocrain2 = spark.sql (\n",
    "    \"\"\" \n",
    "    SELECT *\n",
    "    FROM Pregle\n",
    "    WHERE id = 'EwTZ2xpQwpA'\n",
    "    \"\"\")\n",
    "\n",
    "chocrain2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------------------+\n",
      "|         id|outDegree|                rank|\n",
      "+-----------+---------+--------------------+\n",
      "|EwTZ2xpQwpA|       40|8.456323908269459E-6|\n",
      "+-----------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chocrain4 = spark.sql (\n",
    "    \"\"\" \n",
    "    SELECT *\n",
    "    FROM Pregle2008\n",
    "    WHERE id = 'EwTZ2xpQwpA'\n",
    "    \"\"\")\n",
    "\n",
    "chocrain4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "chocrain3 = spark.sql (\n",
    "    \"\"\" \n",
    "    SELECT *\n",
    "    FROM results2008\n",
    "    WHERE id = 'EwTZ2xpQwpA'\n",
    "    \"\"\")\n",
    "\n",
    "chocrain3.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numa Numa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|         id|         pagerank|\n",
      "+-----------+-----------------+\n",
      "|KmtzQCSh6xk|8.354997214483403|\n",
      "+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numa1 = spark.sql (\n",
    "    \"\"\" \n",
    "    SELECT *\n",
    "    FROM results\n",
    "    WHERE id = 'KmtzQCSh6xk'\n",
    "    \"\"\")\n",
    "\n",
    "numa1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------------------+\n",
      "|         id|outDegree|                rank|\n",
      "+-----------+---------+--------------------+\n",
      "|KmtzQCSh6xk|       20|2.311615082751400...|\n",
      "+-----------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numa2 = spark.sql (\n",
    "    \"\"\" \n",
    "    SELECT *\n",
    "    FROM Pregle\n",
    "    WHERE id = 'KmtzQCSh6xk'\n",
    "    \"\"\")\n",
    "\n",
    "numa2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------------------+\n",
      "|         id|outDegree|                rank|\n",
      "+-----------+---------+--------------------+\n",
      "|KmtzQCSh6xk|       40|7.666802140072092E-7|\n",
      "+-----------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numa4 = spark.sql (\n",
    "    \"\"\" \n",
    "    SELECT *\n",
    "    FROM Pregle2008\n",
    "    WHERE id = 'KmtzQCSh6xk'\n",
    "    \"\"\")\n",
    "\n",
    "numa4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table or view not found: results; line 3 pos 9;\n'Project [*]\n+- 'Filter ('id = KmtzQCSh6xk)\n   +- 'UnresolvedRelation [results]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-dfb8189068ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m numa1 = spark.sql (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\" \n\u001b[1;32m      3\u001b[0m     \u001b[0mSELECT\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mFROM\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mWHERE\u001b[0m \u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'KmtzQCSh6xk'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.0.1/libexec/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \"\"\"\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.0.1/libexec/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.0.1/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.0.1/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table or view not found: results; line 3 pos 9;\n'Project [*]\n+- 'Filter ('id = KmtzQCSh6xk)\n   +- 'UnresolvedRelation [results]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "numa3 = spark.sql (\n",
    "    \"\"\" \n",
    "    SELECT *\n",
    "    FROM results2008\n",
    "    WHERE id = 'KmtzQCSh6xk'\n",
    "    \"\"\")\n",
    "\n",
    "numa3.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEEEEEEEERoy JENKINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|pagerank|\n",
      "+---+--------+\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leeroy1 = spark.sql (\n",
    "    \"\"\" \n",
    "    SELECT *\n",
    "    FROM results\n",
    "    WHERE id = 'ufeEAxz1AH8'\n",
    "    \"\"\")\n",
    "\n",
    "leeroy1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-5bf15dac8d2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m leeroy3 = spark.sql (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\" \n\u001b[1;32m      3\u001b[0m     \u001b[0mSELECT\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mFROM\u001b[0m \u001b[0mresults2008\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mWHERE\u001b[0m \u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ufeEAxz1AH8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.0.1/libexec/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \"\"\"\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.0.1/libexec/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.0.1/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.0.1/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;"
     ]
    }
   ],
   "source": [
    "leeroy3 = spark.sql (\n",
    "    \"\"\" \n",
    "    SELECT *\n",
    "    FROM results2008\n",
    "    WHERE id = 'ufeEAxz1AH8'\n",
    "    \"\"\")\n",
    "\n",
    "leeroy3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----+\n",
      "| id|outDegree|rank|\n",
      "+---+---------+----+\n",
      "+---+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leeroy4 = spark.sql (\n",
    "    \"\"\" \n",
    "    SELECT *\n",
    "    FROM Pregle2008\n",
    "    WHERE id = 'ufeEAxz1AH8'\n",
    "    \"\"\")\n",
    "\n",
    "leeroy4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----+\n",
      "| id|outDegree|rank|\n",
      "+---+---------+----+\n",
      "+---+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "leeroy2 = spark.sql (\n",
    "    \"\"\" \n",
    "    SELECT *\n",
    "    FROM Pregle\n",
    "    WHERE id = 'ufeEAxz1AH8'\n",
    "    \"\"\")\n",
    "\n",
    "leeroy2.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The missing piece \n",
    "https://www.youtube.com/watch?v=oHg5SJYRHA0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------------------+\n",
      "|         id|outDegree|                rank|\n",
      "+-----------+---------+--------------------+\n",
      "|oHg5SJYRHA0|       56|1.887901064230858...|\n",
      "+-----------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A = spark.sql (\n",
    "    \"\"\" \n",
    "    SELECT *\n",
    "    FROM Pregle2008\n",
    "    WHERE id = 'oHg5SJYRHA0'\n",
    "    \"\"\")\n",
    "\n",
    "A.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
